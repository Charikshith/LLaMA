# LLaMA

![LLaMA_Final_page-0017](https://github.com/user-attachments/assets/053b434f-f6d5-4886-97a8-5ba731af776f)

![LLaMA_Final_page-0018](https://github.com/user-attachments/assets/1edc58f8-0d55-4af1-80aa-fbf4da1ee0dc)

![LLaMA_Final_page-0019](https://github.com/user-attachments/assets/8f651b56-37fa-442b-b2ef-34dc70102437)

- In Relative Positional Encoding we have 3 vectors.
- Token1, Token2 and its Distance(a_ij).
![LLaMA_Final_page-0020](https://github.com/user-attachments/assets/2f59695c-da20-4fd7-ac82-9197636aa76d)

![LLaMA_Final_page-0021](https://github.com/user-attachments/assets/aeeedf2d-ac11-47e5-a526-2d7cdd7ccc07)
- m & n are scalar represents the postion of the words for query and key.
- 

![LLaMA_Final_page-0022](https://github.com/user-attachments/assets/8f9c0018-0104-4f8e-a5d9-228c99e5033d)

![LLaMA_Final_page-0023](https://github.com/user-attachments/assets/72550789-4e6b-4bca-9e72-2c717ee27444)

![LLaMA_Final_page-0024](https://github.com/user-attachments/assets/90834de3-43d4-4024-ac77-9482b9ed7a88)

![LLaMA_Final_page-0025](https://github.com/user-attachments/assets/37eb3e8b-8bc1-40be-90d4-215b8a596af3)

![LLaMA_Final_page-0026](https://github.com/user-attachments/assets/dcc06a6d-edda-4b38-b77c-2ff03465d73a)

![LLaMA_Final_page-0027](https://github.com/user-attachments/assets/838d0175-ff55-4e10-93c9-ed60c2901532)
- Attention mechanism is a score that tells how much strong is the relationship between two tokens. The strenght changes wrt token and position of the token as well.
- Where as the Rotary Positional Embeddings are applied to Q & K.
![LLaMA_Final_page-0028](https://github.com/user-attachments/assets/d4c8a8fc-46df-4fb0-bf63-b8742191ef9c)

![LLaMA_Final_page-0029](https://github.com/user-attachments/assets/9945226e-5719-4a54-8b8c-39a4e571782e)

![LLaMA_Final_page-0030](https://github.com/user-attachments/assets/221ca1b4-127f-4c0f-9b8b-3b595ee3ac22)









